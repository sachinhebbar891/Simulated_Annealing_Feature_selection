# -*- coding: utf-8 -*-
"""Simulated_Annealing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bxkFC9ZXAFYtmY71YwBsChrkwYhTkazW
"""

import pandas as pd
import numpy as np
import random
import time
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from xgboost.sklearn import XGBRegressor
from sklearn.metrics import make_scorer, accuracy_score, mean_squared_error, r2_score, mean_absolute_percentage_error

data = pd.read_csv('/content/Housing.csv')
data.head()

data.isna().sum()

data.describe(include='all')

data['furnishingstatus'].value_counts()

data.replace({'yes':1, 'no':0}, inplace = True)

data.head()

data2 = pd.get_dummies(data, columns=['furnishingstatus'])

data2.head()

data2.replace({True:1, False: 0}, inplace = True)
data2.head()

X = data2.drop(['price'], axis = 1)
y = data2['price'].copy()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

xgb_model = XGBRegressor(
    n_estimators=100,      # Number of boosting rounds
    learning_rate=0.1,     # Step size shrinkage used in update to prevent overfitting
    max_depth=4,           # Maximum depth of a tree
    subsample=0.8,         # Subsample ratio of the training instances
    colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree
    random_state=42        # Seed for reproducibility
)

def simulated_annealing_mape(X, y, initial_temp, coolingfactor, beta, max_iter):
  all_solution = []
  all_mape = []

  current_solution = [random.randint(0, 1) for _ in range(0, len(X.columns))]
  features = X.columns
  selected_features = [features[i] for i in range(0, len(X.columns)) if current_solution[i] == 1]
  X_feat = X[selected_features]
  cv_scores = cross_val_score(xgb_model, X_feat, y, cv=5, scoring='neg_mean_absolute_percentage_error')
  current_mape = -np.mean(cv_scores)

  all_solution.append(current_solution)
  all_mape.append(current_mape)

  for iter_num in range(0, max_iter):
    if iter_num == 0:
      temp = initial_temp
    elif iter_num == beta:
      temp = initial_temp*(coolingfactor**iter_num)

    k = random.choice(range(0,len(current_solution)))
    new_solution = current_solution.copy()
    new_solution[k] = 1 - new_solution[k]

    selected_features = [features[i] for i in range(0, len(X.columns)) if new_solution[i] == 1]
    X_feat = X[selected_features]
    cv_scores = cross_val_score(xgb_model, X_feat, y, cv=5, scoring='neg_mean_absolute_percentage_error')
    new_mape = -np.mean(cv_scores)

    if new_mape < current_mape:
      current_solution = new_solution.copy()
      current_mape = new_mape

      all_solution.append(current_solution)
      all_mape.append(current_mape)

    elif random.random() < np.exp(-abs(new_mape - current_mape) / temp):
      current_solution = new_solution.copy()
      current_mape = new_mape

      all_solution.append(current_solution)
      all_mape.append(current_mape)

    else:
      all_solution.append(current_solution)
      all_mape.append(current_mape)
      continue

  return all_solution, all_mape

start = time.time()
max_iter = 1000
all_solution, all_mape = simulated_annealing_mape(X_train, y_train, 10, 0.95, 50, max_iter)
end = time.time()
print("Time taken:", (end-start)/60)

sns.lineplot(x=range(0,max_iter + 1), y=all_mape)

print(len(all_solution), len(all_mape))

best_mape = np.min(all_mape)
print(best_mape)

best_solution = [all_solution[i] for i in range(0, len(all_solution)) if all_mape[i] == best_mape][0]

best_solution

features = X_train.columns
selected_features = [features[i] for i in range(0, len(features)) if best_solution[i] == 1]
X_feat = X_train[selected_features]
cv_scores = cross_val_score(xgb_model, X_feat, y_train, cv=5, scoring='neg_mean_absolute_percentage_error')
mape = -np.mean(cv_scores)
print(mape)

xgb_model.fit(X_train[selected_features], y_train)
y_pred = xgb_model.predict(X_test[selected_features])
mape = mean_absolute_percentage_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)
mse = mean_squared_error(y_test, y_pred)
print("mape:",mape)
print("mse:",mse)
print("R squared:",r2)
print("Adjusted R squared:",adjusted_r2)

